import React, { useEffect, useMemo, useRef, useState, useCallback } from "react";
import { View, StyleSheet, Text } from "react-native";
import { Camera, useCameraDevice } from "react-native-vision-camera";
import RNFS from "react-native-fs";
import Sound from "react-native-sound";
import RNFetchBlob from 'react-native-blob-util';

export default function CameraScreen() {
    const cameraRef = useRef<Camera>(null);
    const device = useCameraDevice("back");
    const soundRef = useRef<Sound | null>(null); // âœ¨ 1. useStateë¥¼ useRefë¡œ ë³€ê²½

    const [hasPermission, setHasPermission] = useState<boolean | null>(null);
    const [isProcessing, setIsProcessing] = useState(false);
    // const [currentSound, setCurrentSound] = useState<Sound | null>(null); // âŒ ì œê±°
    const [isActive, setIsActive] = useState(true);
    const [recognizedText, setRecognizedText] = useState<string | null>(null);
    const [isCameraReady, setIsCameraReady] = useState(false);

     const cameraConfig = useMemo(() => {
        if (device?.formats == null) return undefined;
        const targetFps = 1;

        // 1. FPSë¥¼ ì§€ì›í•˜ëŠ” í¬ë§· í•„í„°ë§
        const supportingFormats = device.formats.filter(
            (f) => f.minFps <= targetFps && f.maxFps >= targetFps
        );

        if (supportingFormats.length > 0) {
            // 2. í•´ìƒë„(photoWidth)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
            const sortedFormats = supportingFormats.sort(
                (a, b) => a.photoWidth - b.photoWidth
            );
            // 3. ê°€ì¥ ë‚®ì€ í•´ìƒë„ í¬ë§·ì„ ì„ íƒ
            return {
                format: sortedFormats[0],
                fps: targetFps,
            };
        }
        
        // ë§Œì•½ ì ì ˆí•œ FPSì˜ í¬ë§·ì´ ì—†ë‹¤ë©´, ê·¸ëƒ¥ ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë§· ì¤‘ ê°€ì¥ ë‚®ì€ í•´ìƒë„ë¥¼ ì„ íƒ
        const lowestFpsFormat = device.formats.sort((a, b) => a.photoWidth - b.photoWidth)[0];
        return { format: lowestFpsFormat, fps: lowestFpsFormat.minFps };
    }, [device?.formats]);

    useEffect(() => {
        (async () => {
            const status = await Camera.requestCameraPermission();
            setHasPermission(status === "granted");
        })();
    }, []);

    const captureAndSend = useCallback(async () => {
    if (!isCameraReady || isProcessing || cameraRef.current == null) return;
    setIsProcessing(true);

    try {
        const photo = await cameraRef.current.takePhoto({});

        // âœ¨ 2. react-native-blob-utilì„ ì‚¬ìš©í•´ ì„œë²„ì— ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤.
        // ìš”ì²­ ë°©ì‹ì€ FormDataë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ì „ ë‹µë³€ê³¼ ë™ì¼í•©ë‹ˆë‹¤.
        const resp = await RNFetchBlob.fetch('POST', 
            'http://myapp.210-178-40-54.nip.io/ocr-read', 
            {
                'Content-Type' : 'multipart/form-data',
            }, [
                { name : 'image', filename : 'photo.jpg', type:'image/jpeg', data: RNFetchBlob.wrap(photo.path) }
            ]
        );

        // âœ¨ 3. ì‘ë‹µ ì²˜ë¦¬ ë°©ì‹ì´ ì™„ì „íˆ ë°”ë€ë‹ˆë‹¤.
        // ì‘ë‹µ ìƒíƒœê°€ 204ì´ë©´ í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²ƒì´ë¯€ë¡œ ì—¬ê¸°ì„œ í•¨ìˆ˜ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.
        if (resp.info().status === 204) {
            console.log("í…ìŠ¤íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.");
            setIsProcessing(false); // isProcessing ìƒíƒœë¥¼ ê¼­ falseë¡œ ë°”ê¿”ì¤˜ì•¼ í•©ë‹ˆë‹¤.
            return;
        }

        // ì„œë²„ ì˜¤ë¥˜ í™•ì¸
        if (resp.info().status !== 200) {
            throw new Error(`ì„œë²„ ì˜¤ë¥˜: ${resp.info().status}`);
        }

        // âš ï¸ ì¤‘ìš”: ì„œë²„ê°€ í…ìŠ¤íŠ¸ë¥¼ ë³´ë‚´ì£¼ì§€ ì•Šìœ¼ë¯€ë¡œ, í…ìŠ¤íŠ¸ë¥¼ í™”ë©´ì— í‘œì‹œí•˜ëŠ” ì½”ë“œëŠ” ì œê±°í•©ë‹ˆë‹¤.
        // setRecognizedText(data.text);
        // setTimeout(() => setRecognizedText(null), 3000);

        // âœ¨ 4. base64 ë°ì´í„°ë¥¼ ì§ì ‘ ë°›ì•„ì˜µë‹ˆë‹¤.
        // .base64() ë©”ì„œë“œë¡œ MP3 íŒŒì¼ ë°ì´í„°ë¥¼ ë°”ë¡œ base64 ë¬¸ìì—´ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        const audioBase64 = resp.base64();

        const outPath = `${RNFS.CachesDirectoryPath}/ocr_tts.mp3`;
        await RNFS.writeFile(outPath, audioBase64, "base64");
        
        // --- ì‚¬ìš´ë“œ ì¬ìƒ ë¶€ë¶„ì€ ê¸°ì¡´ê³¼ ë™ì¼ ---
        if (soundRef.current) {
            soundRef.current.stop(() => soundRef.current?.release());
        }

        const sound = new Sound(outPath, "", (error) => {
            if (error) {
                console.log('failed to load the sound', error);
                return;
            }
            sound.play(() => sound.release());
        });
        soundRef.current = sound;

    } catch (err: any) {
        if (String(err).includes("Camera is closed")) {
            console.log("ğŸ“· Camera already closed, ignore");
        } else {
            console.error("ìº¡ì²˜ ë° ì „ì†¡ ì˜¤ë¥˜:", err);
        }
    } finally {
        setIsProcessing(false);
    }
}, [isProcessing, isCameraReady]);

    useEffect(() => {
        let cancelled = false;
        const loop = async () => {
            if (!device || hasPermission !== true || cancelled) return;
            await captureAndSend();
            if (!cancelled) setTimeout(loop, 10000);
        };
        loop();

        return () => {
            cancelled = true;
            // âœ¨ 3. soundRef.currentë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë¦¬
            if (soundRef.current) {
                soundRef.current.stop(() => soundRef.current?.release());
            }
        };
    }, [device, hasPermission, captureAndSend]);

    if (hasPermission === null || !device || !cameraConfig) {
        return <View style={styles.center}><Text>ì¹´ë©”ë¼ ë¡œë”© ì¤‘...</Text></View>;
    }
    if (hasPermission === false) {
        return <View style={styles.center}><Text>ì¹´ë©”ë¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤</Text></View>;
    }

    return (
        <View style={{ flex: 1 }}>
            <Camera
                ref={cameraRef}
                style={StyleSheet.absoluteFill}
                device={device}
                isActive={isActive}
                photo={true}
                format={cameraConfig.format}
                fps={cameraConfig.fps}
                onInitialized={() => {
                    console.log("Camera ready!");
                    setIsCameraReady(true);
                }}
            />
            {isProcessing && (
                <View style={styles.overlay}>
                    <Text style={styles.processingText}>ì¸ì‹ì¤‘...</Text>
                </View>
            )}
            {recognizedText && (
                <View style={styles.textOverlay}>
                    <Text style={styles.textDisplay}>{recognizedText}</Text>
                </View>
            )}
        </View>
    );
}

const styles = StyleSheet.create({
  center: { flex: 1, justifyContent: "center", alignItems: "center" },
  overlay: {
    ...StyleSheet.absoluteFillObject,
    justifyContent: "center",
    alignItems: "center",
    backgroundColor: "rgba(0,0,0,0.3)",
  },
  processingText: { fontSize: 24, fontWeight: "bold", color: "#fff" },
  textOverlay: {
    position: "absolute",
    bottom: 40,
    left: 20,
    right: 20,
    backgroundColor: "rgba(0,0,0,0.6)",
    padding: 10,
    borderRadius: 8,
    alignItems: "center",
  },
  textDisplay: {
    color: "#fff",
    fontSize: 18,
    fontWeight: "bold",
    textAlign: "center",
  },
});