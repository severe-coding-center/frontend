import React, { useEffect, useMemo, useRef, useState, useCallback } from "react";
import { View, StyleSheet, Text } from "react-native";
import { Camera, useCameraDevice } from "react-native-vision-camera";
import RNFS from "react-native-fs";
import Sound from "react-native-sound";
import RNFetchBlob from 'react-native-blob-util';
import Config from "react-native-config";
import AsyncStorage from "@react-native-async-storage/async-storage";

const BASE_URL = Config.BACKEND_URL;

export default function CameraScreen() {
    const cameraRef = useRef<Camera>(null);
    const device = useCameraDevice("back");
    const soundRef = useRef<Sound | null>(null); // âœ¨ 1. useStateë¥¼ useRefë¡œ ë³€ê²½

    const [hasPermission, setHasPermission] = useState<boolean | null>(null);
    const [isProcessing, setIsProcessing] = useState(false);
    const [isActive, setIsActive] = useState(true);
    const [isCameraReady, setIsCameraReady] = useState(false);

     const cameraConfig = useMemo(() => {
        if (device?.formats == null) return undefined;
        const targetFps = 1;

        // 1. FPSë¥¼ ì§€ì›í•˜ëŠ” í¬ë§· í•„í„°ë§
        const supportingFormats = device.formats.filter(
            (f) => f.minFps <= targetFps && f.maxFps >= targetFps
        );

        if (supportingFormats.length > 0) {
            // 2. í•´ìƒë„(photoWidth)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
            const sortedFormats = supportingFormats.sort(
                (a, b) => a.photoWidth - b.photoWidth
            );
            // 3. ê°€ì¥ ë‚®ì€ í•´ìƒë„ í¬ë§·ì„ ì„ íƒ
            return {
                format: sortedFormats[0],
                fps: targetFps,
            };
        }
        
        // ë§Œì•½ ì ì ˆí•œ FPSì˜ í¬ë§·ì´ ì—†ë‹¤ë©´, ê·¸ëƒ¥ ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë§· ì¤‘ ê°€ì¥ ë‚®ì€ í•´ìƒë„ë¥¼ ì„ íƒ
        const lowestFpsFormat = device.formats.sort((a, b) => a.photoWidth - b.photoWidth)[0];
        return { format: lowestFpsFormat, fps: lowestFpsFormat.minFps };
    }, [device?.formats]);

    useEffect(() => {
        (async () => {
            const status = await Camera.requestCameraPermission();
            setHasPermission(status === "granted");
        })();
    }, []);

    const captureAndSend = useCallback(async (): Promise<void> => {
    if (!isCameraReady || isProcessing || cameraRef.current == null) return;
    setIsProcessing(true);

    try {
        const photo = await cameraRef.current.takePhoto({});
        console.log('ë°©ê¸ˆ ì°ì€ ì‚¬ì§„ ê²½ë¡œ:', photo.path);
        const accessToken = await AsyncStorage.getItem('accessToken');

        // âœ¨ 2. react-native-blob-utilì„ ì‚¬ìš©í•´ ì„œë²„ì— ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤.
        // ìš”ì²­ ë°©ì‹ì€ FormDataë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ì „ ë‹µë³€ê³¼ ë™ì¼í•©ë‹ˆë‹¤.
        const ocrResp = await RNFetchBlob.fetch('POST', 
            `${BASE_URL}/api/ocr/upload`, 
            {
                'Content-Type' : 'multipart/form-data',
                ...(accessToken && { 'Authorization': `Bearer ${accessToken}` })
            }, [
                { name : 'image', filename : 'photo.jpg', type:'image/jpeg', data: RNFetchBlob.wrap(photo.path) }
            ]
        );

        // 1. (ì¶”ê°€) í…ìŠ¤íŠ¸ê°€ ì—†ì„ ë•Œ (204 No Content) ì²˜ë¦¬
        if (ocrResp.info().status === 204) {
            console.log("í…ìŠ¤íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.");
            return; // ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì—†ì´ í•¨ìˆ˜ ì¢…ë£Œ
        }

        // ì„œë²„ ì˜¤ë¥˜ í™•ì¸
        if (ocrResp.info().status !== 200) {
            throw new Error(`ì„œë²„ ì˜¤ë¥˜: ${ocrResp.info().status}`);
        }

        const audioBase64 = ocrResp.base64();

        const outPath = `${RNFS.CachesDirectoryPath}/ocr_tts.mp3`;
        await RNFS.writeFile(outPath, audioBase64, "base64");
        
        // --- ì‚¬ìš´ë“œ ì¬ìƒ ë¶€ë¶„ì€ ê¸°ì¡´ê³¼ ë™ì¼ ---
        if (soundRef.current) {
            soundRef.current.stop(() => soundRef.current?.release());
        }

        return new Promise((resolve, reject) => {
                const sound = new Sound(outPath, "", (error) => {
                    if (error) {
                        console.log('failed to load the sound', error);
                        reject(error); // ë¡œë”© ì‹¤íŒ¨ ì‹œ Promiseë¥¼ rejectí•©ë‹ˆë‹¤.
                        return;
                    }
                    
                    // ì¬ìƒì´ ëë‚˜ë©´ Promiseë¥¼ resolve()í•˜ì—¬ ë£¨í”„ê°€ ë‹¤ìŒìœ¼ë¡œ ì§„í–‰ë˜ê²Œ í•©ë‹ˆë‹¤.
                    sound.play((success) => {
                        if (success) {
                            console.log('successfully finished playing');
                        } else {
                            console.log('playback failed due to audio decoding errors');
                        }
                        sound.release(); // ì‚¬ìš´ë“œ ë¦¬ì†ŒìŠ¤ í•´ì œ
                        resolve(); // ì¬ìƒ ì™„ë£Œ!
                    });
                });
                soundRef.current = sound;
            });

    } catch (err: any) {
        if (String(err).includes("Camera is closed")) {
            console.log("ğŸ“· Camera already closed, ignore");
        } else {
            console.error("ìº¡ì²˜ ë° ì „ì†¡ ì˜¤ë¥˜:", err);
        }
    } finally {
        setIsProcessing(false);
    }
}, [isProcessing, isCameraReady]);

    useEffect(() => {
        let cancelled = false;
        const loop = async () => {
            if (!device || hasPermission !== true || cancelled) return;
            await captureAndSend();
            if (!cancelled) setTimeout(loop, 5000);
        };
        loop();

        return () => {
            cancelled = true;
            // âœ¨ 3. soundRef.currentë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë¦¬
            if (soundRef.current) {
                soundRef.current.stop(() => soundRef.current?.release());
            }
        };
    }, [device, hasPermission, captureAndSend]);

    if (hasPermission === null || !device || !cameraConfig) {
        return <View style={styles.center}><Text>ì¹´ë©”ë¼ ë¡œë”© ì¤‘...</Text></View>;
    }
    if (hasPermission === false) {
        return <View style={styles.center}><Text>ì¹´ë©”ë¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤</Text></View>;
    }

    return (
        <View style={{ flex: 1 }}>
            <Camera
                ref={cameraRef}
                style={StyleSheet.absoluteFill}
                device={device}
                isActive={isActive}
                photo={true}
                format={cameraConfig.format}
                fps={cameraConfig.fps}
                onInitialized={() => {
                    console.log("Camera ready!");
                    setIsCameraReady(true);
                }}
            />
            {isProcessing && (
                <View style={styles.overlay}>
                    <Text style={styles.processingText}>ì¸ì‹ì¤‘...</Text>
                </View>
            )}
        </View>
    );
}

const styles = StyleSheet.create({
  center: { flex: 1, justifyContent: "center", alignItems: "center" },
  overlay: {
    ...StyleSheet.absoluteFillObject,
    justifyContent: "center",
    alignItems: "center",
    backgroundColor: "rgba(0,0,0,0.3)",
  },
  processingText: { fontSize: 24, fontWeight: "bold", color: "#fff" },
  textOverlay: {
    position: "absolute",
    bottom: 40,
    left: 20,
    right: 20,
    backgroundColor: "rgba(0,0,0,0.6)",
    padding: 10,
    borderRadius: 8,
    alignItems: "center",
  },
  textDisplay: {
    color: "#fff",
    fontSize: 18,
    fontWeight: "bold",
    textAlign: "center",
  },
});